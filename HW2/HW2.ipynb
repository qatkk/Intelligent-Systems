{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2.ipynb","provenance":[],"collapsed_sections":["Qk2kqqZE6Y8Z","KH8MMPN30QNL","xH4K8_mYKIpp","6LyY-SJ96RVp","nBG9mJUCxkCf","yvMd-GIOf-ra","In2c9sYD3PtH","6KtcOfRo6Jh5","USFNOkAB4KWf","rm3-baRy4N5U"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VnosiVfnLTjf","executionInfo":{"status":"ok","timestamp":1652198409402,"user_tz":-270,"elapsed":1595,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["import numpy as np \n","import matplotlib as plt \n","import pandas as pd\n","from sklearn.utils import shuffle"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk2kqqZE6Y8Z"},"source":["# First Problem"]},{"cell_type":"markdown","metadata":{"id":"KH8MMPN30QNL"},"source":["\n","## Decision Tree with Information Gain \n","### Building The Tree \n","As we know we can build a decision tree with multiple metrics. In this question we implement ID3 to build our desicion tree. ID3 uses information gain at each level to determine which feature splits the data best. Information gain is calculated with the help of entropy. Entropy finely describes how much a data is disordered, on the other hand, information gain shows the order of our data. In each level of building the tree we compute the information gain for remaining features and choose the feature with maximum information gain as our decision node. \n","The equations for entropy and information gain is as below. \n","$$ E = -Σ p_i log_2 p_i $$\n","$$ IG = E_{set} - Σ E_{subset} * p_{subset}$$\n"]},{"cell_type":"code","metadata":{"id":"lRPcCjGo0WoD","executionInfo":{"status":"ok","timestamp":1652198410016,"user_tz":-270,"elapsed":20,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["def compute_entropy (len_1, len_2,  len_3) :\n","  len_total = len_1 + len_2 + len_3\n","  param_one = len_1/len_total if len_1 else 1\n","  param_two = len_2/len_total if len_2 else 1\n","  param_three = len_3/len_total if len_3 else 1\n","  # Error handling : if a parameter is zero it will be ignored in calculation \n","  #  since we set it's value to 1 in a log\n","  entropy =  - param_one * np.log2(param_one) - param_two* np.log2(param_two) - param_three * np.log2(param_three)\n","  return entropy\n","def compute_information_gain (source, sub) : \n","  sub_entropy = []\n","  sub_entropy.append(compute_entropy(len(np.where(source[np.where(sub == 1 )[0]] == 1 )[0]), len(np.where(source[np.where(sub == 1 )[0]] == 0 )[0]), len(np.where(source[np.where(sub == 1 )[0]] == 2 )[0])))\n","  #  Get the entropy of the feature's subset \n","  sub_entropy.append(len(np.where(sub == 1 )[0])/ len(source))\n","  #  Compute it's probability \n","  sub_entropy.append(compute_entropy(len(np.where(source[np.where(sub == 0 )[0]] == 1 )[0]), len(np.where(source[np.where(sub == 0 )[0]] == 0 )[0]), len(np.where(source[np.where(sub == 0 )[0]] == 2 )[0])))\n","  sub_entropy.append(len(np.where(sub == 0 )[0])/ len(source))\n","  sub_entropy.append(compute_entropy(len(np.where(source[np.where(sub == 2 )[0]] == 1 )[0]), len(np.where(source[np.where(sub == 2 )[0]] == 0 )[0]), len(np.where(source[np.where(sub == 2 )[0]] == 2 )[0])))\n","  sub_entropy.append(len(np.where(sub == 2 )[0])/ len(source))\n","  sv_entropy = 0\n","  s_entropy = compute_entropy(len(np.where(source == 1 )[0]), len(np.where(source == 0 )[0]), len(np.where(source == 2 )[0]))\n","  for i in range(0, len(sub_entropy), 2) : \n","    sv_entropy += sub_entropy[i]* sub_entropy[i+1]\n","  #  Each subset's entropy is multiplied with it's probability  \n","  information_gain = s_entropy - sv_entropy\n","  return information_gain\n","values = [np.array([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]), np.array([0, 0, 2, 1, 2, 1, 1, 0, 2, 0, 2, 1, 0, 1])]\n","values = np.concatenate((values,[np.array([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0])]), axis  = 0 )\n","values = np.concatenate((values,[np.array([1, 0, 1, 1, 2, 0, 2, 1, 2, 1, 0, 1, 1, 2])]), axis  = 0 )\n","values = np.concatenate((values,[np.array([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0])]), axis  = 0 )\n","#  Importing table's data \n","headers = [\"pressure\", \"cholesterol\", \"smoking\", \"weight\"]\n","def build_tree (source_data, attributes, branch, depth, headers, parent): \n","  max_depth = 3\n","  tree  = {}\n","  IGs = []\n","  IGs = [compute_information_gain(source_data, attributes[i]) for i in range(len(attributes))]\n","  # IGs is the information gain of all the remaining attributes \n","  best_attribute = np.argmax(IGs)\n","  #  The best attribute's index\n","  tree [\"name\"] = headers[best_attribute]\n","  if (depth == max_depth or len(attributes) == 1 ): \n","    n_label = [] \n","    label = []\n","    for value in np.unique(source_data) : \n","        n_label.append(np.count_nonzero(source_data == value))\n","        label.append(value)\n","    tree[\"lable\"] =  label[np.argmax(n_label)]\n","    #  If the tree ends with this conditions the lable of the data \n","    #  will be the majority's lable \n","    return tree\n","    #  Our tree ends whether we have only one attribute left or we have \n","    #  encountered the maximum depth \n","    #  at this point, function returns the tree at hand\n","  else :\n","    if (IGs[best_attribute] == 0 ) : \n","      tree[\"lable\"] =  source_data[0]\n","      #  when the IG of the best attribute is 0 it means \n","      #  all the values in that attribute are the same \n","      #  so lable can be any of them. \n","      if (branch != -1) : \n","        tree[\"branch\"] = branch\n","    else : \n","      tree [\"children\"] = []\n","      for i in np.unique(attributes[best_attribute]) : \n","        lables = []\n","        names = [name for name in headers if name != headers[best_attribute]]\n","        for j in np.where(attributes[best_attribute] == i)[0] :\n","          lables.append([ row[j] for row in np.delete(attributes, best_attribute, 0)])\n","        sub_tree = build_tree(source_data[np.where(attributes[best_attribute] == i )[0]], np.asarray(lables).T, i, depth+1, names, tree)\n","        if (branch == -1 or branch >= i  ): \n","          tree[\"branch\"] = i\n","        tree[\"children\"].append(sub_tree)\n","        #  Each returned sub_tree will be a child of the tree we have \n","  return tree"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7c0LMDgQiQq","executionInfo":{"status":"ok","timestamp":1652198410017,"user_tz":-270,"elapsed":19,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}},"outputId":"1fc5f5d1-d159-42c9-cbb3-6d5db589ad25"},"source":["decision_tree = build_tree(values[4], np.delete(values, 4, 0), -1 , 0, headers, {})\n","print (decision_tree)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'cholesterol', 'children': [{'name': 'pressure', 'children': [{'name': 'smoking', 'lable': 0, 'branch': 0}, {'name': 'smoking', 'lable': 1, 'branch': 1}], 'branch': 0}, {'name': 'smoking', 'children': [{'name': 'pressure', 'lable': 0, 'branch': 0}, {'name': 'pressure', 'lable': 1, 'branch': 1}], 'branch': 1}, {'name': 'pressure', 'lable': 1, 'branch': 2}], 'branch': 2}\n"]}]},{"cell_type":"markdown","metadata":{"id":"kINQw8fZNG8z"},"source":["### Prediction \n","Now that we have build the tree with the train data, using it we can predict the labels of the test data. Each data according to the values of it's features goes through the tree and is assigned to the lable of the leaf it ends up to. \n","With the description above, predicted labels and the final decision tree are as below.\n","\n","![Q1_treee.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gIoSUNDX1BST0ZJTEUAAQEAAAIYAAAAAAIQAABtbnRyUkdCIFhZWiAAAAAAAAAAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAAHRyWFlaAAABZAAAABRnWFlaAAABeAAAABRiWFlaAAABjAAAABRyVFJDAAABoAAAAChnVFJDAAABoAAAAChiVFJDAAABoAAAACh3dHB0AAAByAAAABRjcHJ0AAAB3AAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAFgAAAAcAHMAUgBHAEIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFhZWiAAAAAAAABvogAAOPUAAAOQWFlaIAAAAAAAAGKZAAC3hQAAGNpYWVogAAAAAAAAJKAAAA+EAAC2z3BhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABYWVogAAAAAAAA9tYAAQAAAADTLW1sdWMAAAAAAAAAAQAAAAxlblVTAAAAIAAAABwARwBvAG8AZwBsAGUAIABJAG4AYwAuACAAMgAwADEANv/bAEMAAwICAgICAwICAgMDAwMEBgQEBAQECAYGBQYJCAoKCQgJCQoMDwwKCw4LCQkNEQ0ODxAQERAKDBITEhATDxAQEP/bAEMBAwMDBAMECAQECBALCQsQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEP/AABEIAUYB3wMBIgACEQEDEQH/xAAdAAEBAQADAQEBAQAAAAAAAAAABwYEBQgDAgEJ/8QAUhAAAAYCAQICBAgICQoGAgMAAAECAwQFBgcREiEIExQiMUEVFiMyUXeX1RcYMzdCVmG2OFJXWGJxgZW1JCUnNDVDU3KDtERUY3N0gpGToaKx/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/AP8AVMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAR+B4gb+/csnsQ8Oeysgra64s6QrKJLx5liS/AmvQ5Cm0ybVp4kedHdIjW2kzIiPjuOX+GTYv8ANO2r/eWK/fIeGn83Vv8AWBnf71WgqoCVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fIfhk2L/NO2r/AHliv3yKqACVfhk2L/NO2r/eWK/fI1WsdhQ9oYg3lsKhtaT/ADhZVciutPI9KiyoM1+FIbWcd11o+HoznBocUk08Hz3GrEq8NP5urf6wM7/eq0AVUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABKvDT+bq3+sDO/3qtBVRKvDT+bq3+sDO/3qtBVQAAAAAZiv2hrS2zCbryr2JjEzKq1tTs2ij28dywjIT08qcjpWbiCLrRyZpL5yfpId9W2VdcQWLSosI06FJQTjEiM6l1p1B+xSVpMyUX7SMByQHGn2dbVNtO2dhGhofebjNKfdS2S3nFElDaTUZcqUoyIkl3Mz4IckAAcWXaVte9Ejz7GNGdnvejxEPPJQqQ70qX0NkZ8rV0oWrguT4SZ+wjHKAAAcaHZ1ti5Kar7CNJXBeONKSy6lZsPElKjbWRH6qulST6T4PhRH7yAckAAAEq8NP5urf6wM7/eq0FVEq8NP5urf6wM7/eq0AVUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABKvDT+bq3+sDO/wB6rQVUSrw0/m6t/rAzv96rQVUAHGs0THa2W3XuE3KWw4lhR+xLhpPpP/8APA5IAPNGqs30ovBsB1LYux5mwaF6OqXjUaQSrurtkEr0qbJaQpLrTfUt1bj6+EOoe/TJ5KV4bFCzbBNf4/Z4Dk2TSrW+1ldzvQVznpccpUVUb0VcaIvrbZcbS8tPDKE+Z2Nwlq4Me0AAeO6zIKqzNyFrbPrXNcIjZHhz0ayn3b9wlFuuafpbCJj7i1nwlMdS2erpaUs0klHJpLNa0zvY9pYZEVztOvqLc8Ws3craj5hb3surmk4gmX36tUEmaM2+XU+Wy6nqQZrST3leYXukAHjGgn4xkVdra5fyiW7AqtjGzFsIe0LDIah55ysf6ER7V5TLstBvdDam3iUSXVOMp5So0nj8f2DtOZU5w/J2JGi5C3hVzIySFAzu1tLCvsUKT6O6uvchoj0TjZm4lLTT6ScI+SJ7y/ML38ADyLt6yLBLmXj7+zcuqLOtxiFL15EPJZynb28cffN9s0LdM7NfUUZBx3TdS225ySEEfUVr0oqQu02OuWgkPqy5ZupL2Ev4Ph8l/wDkU8AAAAAEq8NP5urf6wM7/eq0FVEq8NP5urf6wM7/AHqtAFVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASrw0/m6t/rAzv96rQVUSrw0/m6t/rAzv96rQVUAAAAAAAAAAAAAAAAAAAAAEq8NP5urf6wM7/eq0FVEq8NP5urf6wM7/AHqtAFVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASpfhm1d6ZPmwpmwKz4TsJlpIj1ex8igRfSpT65EhxEePOQ011vOuLMkISnqWfbuH4tOuv1j2r9rGVfeIqEuXFgRXp06S1HjR21OvPOrJCG0JLlSlKPsRERGZmfYiISE9s5ftc0wfD7Wxl0zp9L2d27KzqyQZfOrmSNK7FXPsWRtx/eTqzI0AOlz7Wmh9Z1zFhl2d7UjrmuHHr4TO1MskTbF/jkmIsZuwU7IcMv0G0mfHc+CIzGUx/wANuT7Auo2RW+T7W1xi8Z0nY9KjamQzLmzRx29NcXPcZiIP3ss+Y57DN5B8oFswHTmM4RYv5TLlTsly+c35c7JblaXpzqfe23wRNxmefYywltv39JnyZ7wBKvxaddfrHtX7WMq+8Q/Fp11+se1ftYyr7xFVABKvxaddfrHtX7WMq+8RP9l6SxfH801PU1GZbVjxMmzCTV2jf4VMnX6RFRQW0tLfKp5mjh+JHXykyV6nHPSpRH6VEq3J+cXRP1gTP3VvgD8WnXX6x7V+1jKvvEPxaddfrHtX7WMq+8RVQASr8WnXX6x7V+1jKvvEY/YfhOVYsRLLWe2NmVFlXr8xVfZbLyaRW2aPe0+pM8pDR8fNcacLpM+VIdIuk/QoAPMuDYdqnIbxOB5nI29hmck2478A2O3coWUxpHzn4MhNh5cxn3maOFoIy8xtsz4FF/Fp11+se1ftYyr7xG0zjAMP2RRqx3NaNmyheYl9rqUpt2O8nuh5l1Bk4y6k+6XG1JWn3GQnarbamkCMsl+E9j4KyRq+Fo8cnMgp2i9hSI7Rf5xaIv8AespJ8iL1mnT6nAHYfi066/WPav2sZV94ja4HgeM61xljEMQiy2K1iRLlkUuwkTn3H5MhyTIdckSVuPOrW886s1LWo+VmOXi+VY3mtFEybEbyFb1U5HXHlxHicbWXsPuXsMj5IyPuRkZGRGQ7UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH8PsXID+id5vumlxu8Vg2K1E3Ms2NtDnxfqDQa4ra/mPTXlGTUNk/b1OmSlER+Whw/VGfItx7pT6/wlqvDHFfNI0fGaybI/eZdTda0svo65PB+2OohRMJwHDtcUicdwnH4tTB8xT60NEalvvKPlbrrijNbrqj7qcWpSlH3MzAYGHpm9z+Yzf8AiEuYl/5S0PRMRrutOPwHEnylS0rInLB0u3ryC8sjIjQy2fcVxCENoS22hKEIIkpSkuCIi9hEQ/QAAAAAAAACVbk/OLon6wJn7q3wqo6S/wAQpsltsaurRDqpOJ2jlxXGhzpJMhcKTDUai/SLyZj5cfSaT9wDuwAAAAAAAAAS7KNLuxr+VsHT18nDMrmO+k2KEsebVXyyTwRWEUjLqWZERekNGh8uC5WtJdB/XDNztzbxjX+zqFWFZs4jlqDIf82DamXzl1szpSmURF3NsyQ+gu62klwZ0wdJmOE4nsGhexnNKCHcVj6krUxJb6iS4k+UOIP5zbiT7pWkyUkyI0mRlyA7sBJKuv29qeyiVUZ2dsnC5MhuO2uVJbLIKjzFEkjcdcUhudGRz3WtSJCE8mZyD5Mq2AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6/IMgosUpZuSZPcwqmqrmVSJc2a+llhhtPtWtajIkkX0mYCaeKnH132jMsM7+2r2YFXKmOsQH0spm9DSjS08rpNzy+rpUaUKR1dJJUakGpCqpC/1Nj/2kf8A+EOBFkY7neLsS/RWrKkvYSHUtTIhkiRGdQRkTjLqSPhSTLlK0kffgyHapSlKSSkiIiLgiL2EQD+gAAAAAAAAAAAAAg68BwjId6QMn1ricGHcY5avyMtzFEcvSZRqZWn4I9JV8pILlxtSm+TaYS0hJElZJSm8CcStWeHvG89qcyk6vwOBmd7aOtV1wWPxEWMmf6O9IcNMgm/M8w2GJCzV1cmlCu4CjgAAAAAAAAAAAAAy+05tzW6zy2wx01FaRaOc9CNKzSon0sLNBkZdyMlEXBkNQADz7oamxjFNlzaHXcWLEx6dg9JdTWYSSSy7YPOyC9LX0kRKfebTy44fKl9CTUZ8EPQQxchvUuh8VtMhRUUGF4/6V6baSINciLHJ55aUKkyPJQRFypSTW6vskuVLUSSMy18WVGnRmZsKQ1IjyG0utOtLJaHEKLlKkqLsZGRkZGXtAfUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfN99mMy5JkvIaaaSa3HFqJKUJIuTMzPsREXvEef2Tmu4X3KfQ6267HkqJMnYE2MT0RxJKMloqmFdpi+xl6Qv/Jkn3LzzI0EGq2Ht2jwafDxaDXzckzC2QpdZjlWklynkl2851RmSI0cj7KfdNKC9hGpRkk+hx/Ud7ltvCzrfU+HdWsRaX63G4SlLoqVwvmrQhZEcySn/wAy8kuk+7TbPfnW681hiWs4MtjHYr7s20eKTa2s59UmfZyCLjzZL6vWcVx2IuyUF6qEpSRJLWAAAAAAAAAAAAAAAAAACVbk/OLon6wJn7q3wqolW5Pzi6J+sCZ+6t8AqoAAAAAAAAAAAAAAAA/LjbbramnUJWhZGlSVFyRkftIy94j8rU2VaumPZB4fZcRiveeVJsMHsnVJqpXJescBwuTrXTPvwhKo6j56mkqUbpWIAGH11t7GNiPSqRtiZRZVVtIdtsZt0JZsq8lGZJUtslKS40oyMkvtKW0vg+lZ8HxuBjtiapxHZjERd21LhWtW551Xd1kg4tlWufxmH0+sRHwXU2rqbWXqrQtPYYlOzc300oq/fiWbDG09RtbAro3kxGEc+qm1jEZnDXwfeQjmMZkZq9H5JACzgPlFlRp0ZmbCkNSI8htLrTrSyWhxCi5SpKi7GRkZGRl7R9QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGT2Hs/EtZwYj+RSn3Zto8caqqoLCpM+zkEXPlRmE+s4rjuZ9koL1lqSkjUWSyDbl7ltvNwXQsCHdWsRamLHJJqVLoaVwvnIWtBkcuSn/yzKi6T7OuM9ue+17qKjwafMymfYzckzC2QSLPI7RRLlPJLv5LSSIkRo5H3Sw0SUF7TJSjNRhlWNbZruF9u43whuux5KjVG1/Ckk9EcSSuULtX09pi+xH6Oj/Jkn2PzzJKysLDDMZluNGZQ0y0kkNtoSSUoSRcEREXYiIvcPoAAAAAAAAAAAAAAAAAAAAAAJVuT84uifrAmfurfCqiVbk/OLon6wJn7q3wCqgAAAAAAAAAAAAAAAAAAAD8uNtutqadQlaFkaVJUXJGR+0jL3j9AAjsrU2VaumPZB4fZcRiveeVJsMHsnVJqpXJescBwuTrXTPvwhKo6j56mkqUbpanXW3sY2I9KpG2JlFlVW0h22xm3QlmyryUZklS0EpSXGlGRkl9pS2l8H0rPg+NyMdsTVOI7MYiLu2pcO1q3POqruskHFsq1z+Mw+n1iI+C6m1dTay9VaFp7ANiAjCdm5vppRV+/Es2GNp6ja2BXRvJiMI59VNrGIzOGvg+8hHMYzIzV6PySBYosqNOjMzYUhqRHkNpdadaWS0OIUXKVJUXYyMjIyMvaA+oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPM+oNZ1O1JWxchzTMNkPy4mwr+tjphbFv6+OxFZk9LTTbEWY20hKS7ESUEKH+LTrr9Y9q/axlX3iOq8Lv8As7Zn1oZN/wB0LUAlX4tOuv1j2r9rGVfeIfi066/WPav2sZV94iqgAlX4tOuv1j2r9rGVfeI+Urwv6wnRXoM662hIjyG1NPMu7WylaHEKLhSVJOw4MjIzIyPsZCtAAjdH4S9PYxURMfxqVsepq4DRMxIMHaGTsR47ZexDbaLAkoSX0EREOf8Ai066/WPav2sZV94iqgAlX4tOuv1j2r9rGVfeIfi066/WPav2sZV94iqgAlX4tOuv1j2r9rGVfeIyWH4y3rzxVtYfj+T5lJpZ2vZVk9Bu8vtblr0pFlHbS6kp0h7oUSFKTynjsZj0EIrJ/hoV31Xzf8WjALUAAAAAAAAAAAAADD7C21Q4FMg441BnZBllwha6nHKpCXJstKTIlOK6jS2wwkzIlPvKQ2nki6uoySf925sORrzGGnqStatsnvJaKfG6lx020z7J1KjQhSyIzQ0hCHHnV8H0tNOK4MyIjav1nHwKFJsraw+HMvvDTIyG/eb6XZz5F2Qgu/lR2+TS0yk+lCfpUa1KDNt4hvrOi9LzPZLGBQXe6abD47MmWhPt6XrGY0sln7j8mO1xwfC1e0fKZ4WdcW8iBYZFkmy7afVyDlwZb+xLxlcWQbTjJutJYlNttrNp51szSkvUcWn2KMjsAAJOvSuX0Py+ut85pWOp+bEv3G8ggr/Y4UkvSjL/AJJTZ/tH4i7iyLBLCLQ7+x6FQJmvoiQMprHlPUcx9Z8IbdNZeZAdUZkSUvctqUZJQ8tRkkVscW1qqy8rZVNdV0afAnNKjyYslpLrL7Si4UhaFEZKSZGZGRlwYDlAI3hEuz07nsTTV9OdlYlfNuLwWfKfU4/HcaQpx+mdWrk1m20lTsdajNSmUOoUZmySl2QAAAAAAAAAAAAAAef8d1vU7Q2juKbluU7A/wAyZhDq66PV59eVUWLF+LtPINtEeFLaaLl6S+sz6Oo1OHyZjV/i066/WPav2sZV94hpv84u9vrAh/urQiqgJV+LTrr9Y9q/axlX3iH4tOuv1j2r9rGVfeIqoAJQ54ZdbOtqadyDaa0LI0qSra+UmRkftIy+Ee44GP8AhF0xidW3SYs7sSmrmTUpuHX7OyaOwg1GZqMkIsCSXJmZnwXczFmABKvxaddfrHtX7WMq+8Q/Fp11+se1ftYyr7xFVABKvxaddfrHtX7WMq+8Q/Fp11+se1ftYyr7xFVABKvxaddfrHtX7WMq+8Rn6TCoOtfEliVFjWSZq/W3eD5PLnQ7vMre6YcfjT6JLDqW58l5La0JlSEkpBJPh1RHyLqJVkf8KfXn1f5l/iOOAKqAAAAAAAAAAAAAAAAAAACK+F3/AGdsz60Mm/7oWoRXwu/7O2Z9aGTf90LUADo8xzXG8BpF5BlM9yPFJxDDaGIzsqRIeWfCGmGGUrdfdUfsbbQpR8HwR8DvBNt9VFZbYfEO4ocknQ4Vk1MXOxp2QVtTqQlfROitR0qdkKQs0kbKEr6kLVyhxPUhQaTBdiYnseukWWKzZayhSDizIs+uk102I8RErofiym232TNKkqSS0J6kqSouUqIz0o8owcSuNjO1bWQ0eTZVjkjNoS13OT4m3U2lpDbrpKXUz4qIsYzjIcUTKVPR2yWhZp4WkyUrrsZ0nb4jAhydbYLJoMikw84qm5zcRxhxqP57nwVHU6fHlx0cI9HbNSW0Fx5ZJIB6syDJKXFobM+9m+isSJkaA0vy1r6n33UtNI4SRmXUtaS5PsXPJmRdx2Y8V1mAyJMSWjS+tcjwmnW/ibdg3KxqRCJ28ZskLlTSiPIQchbbPT5swupt3pTy44TZ9Py2przKJdbjUS5w1o6ysauIV+3O15Y5eixvlLY8i0RHiSGXTddaStTcxSlpZNam1KbUQD2yAzetoNzWa9xquyKXNlWkWqiszH5zaW5LjyWkkpTqUuupJZmR9RE6suefWV7RpAARWT/DQrvqvm/4tGFqEVk/w0K76r5v+LRgFqAAAAAAAAAAAAASSnaLOfEbf3cnlyBrOrYoq9BnylNpYNolTHDL2dSYvwelJ+0ieeIuCWfNbEp0P8pYbSlufl5GwLDzj95+XGiNN8/9Jpv+zgS/IN155h1BPj0OVQU2TmR5O4kp+K3WVyjjRZakttIh1qkLZYLqQk5DjhNt8JQSFdfKQ9TAPLD26NqJtsj2RTSqJjHqrCsbyiyqrJUiSpxD6ZS32InS62iKs0pIyfUl0lmlKTbLjqLn1viQzzJNkyaSrxqc5jS7ayoFtNYVdJchejIdIrBV0RHXLQp1o0GwRJUjzE8umpKkGHpgBGafZc/DPDJimwbW8xc5h0VUuROzTJ1U8Fxx1DZKU/OUy+aVqNR8ctqNajIuS55HXwdmZRuXTOWXGI5/r2qnQEyGTtMDyhOVoYbTH61El9ceOiPJ7l09TbpJLpVwrqIiDab7xCxzDV9s3jyeMjpSbv8AHnC55RaQlE/GLku/StbZNrL9JtxaT5JRkepwvKa3OcOos2plGcDIKyLaxTM+eWX2kuI//qshw9Zy5U/XGKTp0l2TJk0kF1551ZrW4tTCDUpSj7mZmZmZn3MzGO8LHqeH7C46PyUWCqLH+gmGnlttEX7OhKeP2AKsAAAAAAAAAAAAAlWm/wA4u9vrAh/urQiqiVab/OLvb6wIf7q0IqoD8rWltCnFnwlJGoz+giE2w7xFaszyyrq3HpeSF8Lmoq6ZYYjb10CaZJNXSxMlRW47qjSlSkkhwzUSTNPJEYosxKlRH0pIzM21ERF7TPgec9G6NyBjU+FWOdZ3nU9ymqky4mJ2cWvjRa2b5C0IIksw2pajbJxaUoeec7nyolKJJkHpIB4Yy3StjR6z1XAaweExj7eLvfDdfL1tNyhZ37rUUm5DsGE/HeblklDyUy3OryjIyUps1EZ7J/C7OHsjXc27xOyyfJ4cCkYXLvsJlvSGvKTw8/EvIslyPUqTy4uQw+pz0hSDbSaicJRh61AeMUYHYeVteHiWtJsqxsKKy9JnPYZMpbh+Qb5LRGkznHTiXzi/X8p1jgmUtknk/O799b42ndOfqn22sr+TiNpkmPrdYv8AHZEduXFZrZxOm/GktJUlonVpbWl1BEfJclwtJmHrAB48xHXeaQfEwxbWcA2LWPktjKXYw9dzEvO0Km3CjxnshOaiGuISDYJMRLKnEONo+R9VTo9hgAlWR/wp9efV/mX+I44KqJVkf8KfXn1f5l/iOOAKqAAAAAAAAAAAAAAAAAAACK+F3/Z2zPrQyb/uhahFfC7/ALO2Z9aGTf8AdC1AAAAAAAAAAAAAAAIrJ/hoV31Xzf8AFowtQisn+GhXfVfN/wAWjALUAAAAAAAAAAAAAk+vF/FjeGzsKkeonIHK/Na4j9i23YrcCShP/I7AQtRe45Sf4xDtrjQeq72QzLn0ExLrMibINUW4mxfP9McJyU0+TLySfYcWRGph3qaPgvU7DhbrxfIC+BNs4FWuWGV4Kt99quZNKXLmseSkptaSlcESnCbadb5Mi8+MxyZJ6htcNzDHs+xiuzDFZ6ZlXaM+cw4STSou5kpC0n3Q4hRKQtCiJSVJUkyIyMgGQd8Oun3p8KxXiz5PQYcCuJKLaahqREhdforElsniRKabNxaiQ8S09R9RkauDHPXpHWbmZqz5yhkrtVOOyCQq0lnBRJcbNtySiF5voqJCkGpJvpaJ0yUourufO6AB1+P0NTi1FX41RRfRa2rjNw4jPmKX5bLaSShPUozUfBERcmZn9JjkWECJaQJNZPa82NLZWw8jqNPU2tJpUXJcGXJGfcj5HIABi8+yOr03qO4v4EM/RcWplFXwyUpanVtt9EaOnkzUpS1+W2nk+TNRd+4+uoMLd1zqnD8Ckuk9Ix+jhV0h3nnzX2mUpcc595qWSlf2jDW8gt47Ui4vAQt3B9cWbVjdSyIlMWt8yfVGgIP9JMRzpkOqLsT6I6OTNDqU2gAAAAAAAAAAAAAASrTf5xd7fWBD/dWhFVEq03+cXe31gQ/3VoRVQAAAAAAAAAAAAAAEqyP+FPrz6v8AMv8AEccFVEqyP+FPrz6v8y/xHHAFVAAAAAAAAAAAAAAAAAAABD8d0/vPArHKCwHb2CsVOR5JY5GmNcYHMmyI65bvmKaN5q3YSsk+wj8tJju/i54p/wCWTVX2aWP36KqACAbQvPE3rLB7LMZO1dX2L0ZLbECtY1vYJesZzy0tRYjRne8dbry220mfYjXyfYjGjg4/4snYUdyw25qWPKW0hT7TWubJ1DbhkXUlKzvEmoiPkiUaU8l34L2D8Of6Vt5pZ/K4vqhROL97cvJJDPqp95KKJFd6jL3OzEH2Uz2r4CVfFzxT/wAsmqvs0sfv0Pi54p/5ZNVfZpY/foqoAJV8XPFP/LJqr7NLH79D4ueKf+WTVX2aWP36KqADz7tK48VutMPkZuex9XWldVvMu25Na6skORK83CKRLSn4bV5nkoM3VI9XlCF8HyRJPUsUXigkstyY26tTutOpJbbiNbWKkrSZckZGV9wZGXvFVkR2JTDkWUy28y8g23G3EkpK0mXBpMj7GRl2MhJdIypGC29z4fLh1xR4o2idi77qjUqXjryjJhHUftXFcJcVRFyZNojLV3dAcn4ueKf+WTVX2aWP36PjhmqNkxNvHtvZOxcavJLWNu45Gh0eLSKpCEOSm5BurU9PlGs+W+ngiSXB/sFbAAAAAAAAAAAAAAABKck1pluK5NM2LpOdCjTbFw5F7i9gtTdXeO8ERvpcSlSocsyIiN9CVpXwROtrMkrTVgASmD4kNfwZDdPs8pmtbpSibOJlaExI7rns4jzuTiSSM+ePLdNXs5Skz4FKr7mot4hT6q1hzYyi6iejvpcbMvp6kmZD7yokWdHchzYzUhh5JocadQS0LSftIyPsZCdWHhm8N9tKOda+H3W0ySo+o3pGKQHFmf09SmjMBzcq3xp3C5Ka++2JTJsl8k1VxH/TbF4/oahsEt9w/wBiEGYy8uTtvdaSr62Bb6wwtwy9JsJRpbyK0a97cdlJmVchXfl5wzf4PhLbKuHCpOL4NhOERlQ8Lw+koI6uCU1V17MVB/1pbSRDvAHV4xjGP4ZQQcWxapj1lTWskxFisJ4Q2gv/AOTMzMzNR8mZmZmZmZmO0AAAAAAAAAAAAAAABH16v3RQZvm2S652nhVdW5pcR7t2Dd4TLsX4z7dZCgKSl9m0jJUg0wULIjaIyNai5PsOX8XPFP8Ayyaq+zSx+/RVQAeetq33im1tiZ3UXZuq7i3my49VS1Ra7smTsLCQ4TbDPmfDa/LRyZqW50q8ttDizIySZDXpxzxUmkjXuPVJK47kWtbEyI/6/h0cah/0q7un5c58pjesVv0dN70SL11HTPlF/wCw0ooiT/jOzC9xCvgJV8XPFP8Ayyaq+zSx+/Q+Lnin/lk1V9mlj9+iqgAlXxc8U/8ALJqr7NLH79D4ueKf+WTVX2aWP36KqADzzs++8VGsqaBlE3ZmqZVGVnGiXcsteWLZ1cR4zbKZ0fDZ+YhDqmScLlPQ2pxzkybNJ7H4ueKf+WTVX2aWP36KTcVFZkFTNobqE1Mr7KM7ElxnU8oeZcSaVoUXvI0mZH+wxNNE3NpTt2+kssmPSLzAFNMRZchRG5aUbpK+D5nPPKldCFx3FH3N6K6r2KTyH6+Lnin/AJZNVfZpY/foYvrTaf4U6vZmzNjYpd/AmP2lFChUWJyar/XpMB5x11x6xldfT8HISlKUp/KKMzPgiFVAAAAAAAAAAAAAAAAAAAAAAGN25nzmuMGm5BAgJsbl5bVdR1xmZen2khZNRWO3ckqdWk1qL5jZLWfZJjZCO1iT2zvSTfr+UxfVK3K2uI+6JeRvtcSny5Lv6LGcKOlRHx1ypaT7tkA2mqsCb1rgtdiy5yrCejzJdrYrLhdhYvrN2VJX+1x5a1ce4jJJdiIa4AAAAAAAAAEv3pj9y3XVW1sMr3ZuT6/fcsY8RjjzLOuWkkz69PPYzeaSSkc9vPZYP2EKgADrcayOjzDHazLMZsmbCouYjM+DLZPlD7DqCW2tJ/QaVEY7IR/XJfgr2bcaaf8Ak6G/9JyjDzP5jRKcI7GuSfsLynnSfbQX+6kmlJdLJ8WAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGB3VnNphOGkziqWnctySW3Q4yw4XKV2L5K6XFJ97bLaXZDhf8KO57+BvhHcII9q7huNovH5mO4V6TiuLl7USJvWRWs5PfhRE42iGg+OUnHlcH0ugKBrzB6rW+E0+D0q3XY1TGJk33j6nZLpmanX3D/ScccUtxZ+9S1H7xogAAAAAAAAASXecOZiT1PvqgiPPTMHJ1N3GjtmtyfjzxpOa2SE93HGehEptPtNTBoL8qYrQ/ikpUk0qSRkZcGRl2MgHxgzoVnBj2VdKakxJbSH2H2lkpDraiJSVpUXYyMjIyMvcY+4j+nDLWmVXHh8lmTcCsaVd4Zz2JdG450riJ/+E8smSLnsy7E95mLAAAAAAAAAAAAAAAAAADiyrStgyIkSbYxo78902YjTryULkOEhSzQ2RnytRISpXBcnwkz9hGOUI9s/FaJG6dU5mcRa7ddzKgJfckOLS0x8GTVGhptSjQ11K6TWaEpNfQjqNXQniwgMLubOrHBMJcfxuO1Kye7ktUmNxHPmv2cgzS11/wDptkS3nD9zTLh+4drrfBa7WmD1GE1kh2S3WsGT0t78rMkrUbkiS5/6jry3HV/0nFDCYootrbntM/Ufm45rxUnGsf78okWyuE2cwvp8rhMNJn3SpEwvYsWAAAAAAAAAAAAAAAYDdWCWma4m1NxNxpjMMXlovsZkOGRJTPZSovJWo/mtPtLdjOH/AMN9Zl3Ih3mu84qtk4VU5vTIdaj2jHWqO8XDsV5KjQ9HcL3ONOpW2svcpCiGjEdryXqLdz9Qs/LxLaj7k2CfHCIOSNtdUhnkz4JMthvzkpLt5saSZ8qeIBYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABxV2la3ZNU7ljGTPfZXIaiqeSTzjSDSla0o56jSk1oIzIuCNSefaQ5QjzuK0VP4parIYMRZWN3idsqbJdkOPLWTcmAlttPWo/LbTyoybR0oJS1q6epajOwgJ1vHLruhxeLjGFSEt5hmkxNBQrNPV6M64lSnpqk+9EZhDz5+4zbSn2rIavC8RpMBxKowrG46maykhtQoyVq6lmhCSLqWr2qWrg1KUfdSjMz7mYnWtD/Chsu93Q/wDKUtP6RimHkfzVstukVjPTz/x5LRMpUXY2oaFF2dMV8AAAAAAAAAAAAAATXeWI3drR12d4RFN/McElndU7KVGk56CQaJVerj2pkMKcbIj9VLvkuH3bIbLEMro86xapzLGpZSau6iNTYrnHBm2tJKIlF7UqLnhST7kZGR9yHcCO4YStTbdstZPH0YxnK5WR4t39SLYc9dnXpIi4SSlK9MbL2mbssi7NEAsQ6VrNsMfyt/A2MupXMmjRkzXqVE9o5zUdR8E8qOSvMSgzMuFGnjv7R3Q8q18yC7sqlwZmTHXnVftOzyCwht+rKZqHI8kkTHEc9RR1sLYZS6ZdCl8JI+S4IPVQAAAAAAAAAAAAAJ5vDM7vFsSZpcLW38csumIoMbStHWlqW8lRqlLRyXU1GZQ9JWXvQwZe0yFDEg17/pQ2pebdf+Vosb9IxPEyPuh1SHCKznp9x9b7SYyFF+hFWZdnT5CgYFhVLrnDKbBsdQ6VfSRG4jKnlmt13pL1nXFn3W4tXUtaj7qUpRn3Md+AAAAAAAAAAAAAAAAMltPAWdlYPYYt6euunL8uXV2TSSN2usGFk7FlI/pNuoQrj3kRpPsZjWgAxWoNgP7HwmPcW1eitv4LztVkFagzMoNpHV5clpJn3U31l1NrP57S21l2UQ2ojuVLTqPccDPyUTOL7Fdi4/kJmfDcW4IibrZqjP2ecXEJZ+9Rw/ckxYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEw3zkV43RV2tcKsHYeV7BlKpq+WyfyldF6DXNsC5IyI2I5LUjnsbymEH88hTxINPf6SMwv9+SflIE5Ksew/n2JpmXeXpaf/AJclHmEfvZYin9ICmYxjVJhuOVeJY1XtQamlhswIMZouEssNIJCEF/UkiIdmAAAAAAAAAAAAAAAAMPuLAp2wMLch0E1qBktRJauccnupM0RLSOZqZUsi7m2rlTTif0mnXE+8bgAGV1fn8LZ2D1uYRYL1e9JStmfXPqI366cys2pMR3jt5jTyHG1cdjNHJdjIaoR54y1Hu5D/AHaxTa7xNOcF8nByVpn1FfQhMuM10mZ/76Mgu6n+9hAAAAAAAAAB8pUqLBivTZslqPHjtqdeedWSENoSXKlKUfYiIiMzM/YA+oCPMbS2BtgudEUtfGx1Z9Kc1yJpxUOQXsNUCChSHZifeTq1ssq9qFOkOUnRNpcF52fbw2PePL7rbgW5UMZH7G01qWXCT/zurV9KjAc3emVXdXjsHB8KmKj5fncz4Cp30FyqClSFLlTz+go0dLrpGfqm4TKD/KENpiWLUmD4vVYdjUNMSqpYbUGGyXfoabSSU8n7z4LuZ9zPkz9omL/hX16VzFyaqyvZNfewY70SJaKzu2sJEdl5TanW0enPvoJC1MtGpPT0qNtHJH0kOQ5V+ITXf+VU+RQ9qVDfz6+2Zj1V2SfpalspRDeV9CHGWCPju6XICuAMnr3Z2J7MgS5OOyJDUyrf9EtaucwqNPrJPHPkyGFes2rjuR90rTwpClJMlHrAAAAAAAAAAAAAAAABk9hbOxPWcCJJyKRIdmWj/olVVwWFSZ9nJ458mOwn1nFcdzPslCeVLUlJGog7HNsPo9gYlb4TkjC3ay6iOQ5BNr6HEpWXBLbV7UOJPhSVl3SpKTLuRDKaQy+8vccmYpm0hLmZYVLOjvVEnoKUtCEqYnIT/wAOSwpt4vcSlOI9rZjqW6vxCbE/yq4yKHquoc+ZX1LMe1uzT9Lst5K4bKvpQ2y+Rc9nT4HwY8K+vjuJWS2uWbKsLyfHZiy7NOeW1e/IaaUtTSF+gvsINKDddNKenpSbi+CLkwFjAShWibSnLzsB3hsejeR3Q3Ptyvoy/wBjibJLzhp/5HUK+hRDiv7S2Bqcud70tfJx1B9Ks1x1pxMOOXsJU+CtS3YafebqFvMp9q1NEAsID5RZUWdFZmwpLUiPIbS6y80slocQouUqSouxkZGRkZe0fUAAAAAAAAAAAAAAAAAAAAAABxbS0rKSuk3F1YxYECE0p+TKlPJaZYbSXKlrWoySlJERmZmfBAOUAjzOwdr7WUStP4/DxvGHCPoyzKYjqnJiT44cgVqVNuONmR8pefcaL2Glt1B8nyk6EkWvy2dbq2bfvq7r9Gv1UbKf2IRVJjGSS93UpSvpUZ9wH93zb2FrDqNLYzNdjXmxHXYTshhXDtfStEk7KYR+1Jk0tDCFF3S9KYP2EYpdTVV1FVQ6OnhtQ4FdHbiRY7SeEMstpJKEJL3ESSIi/qEnPwt4NEufjNjua7Lp7wovoSbP472Vk+ljr6/K/wA4uyEGjq9bpNJp578D6vp8QuteJSZsPbNC13fZXHYqsibQRd1Nqb6YUxXYuGzRF9p+ufZICvAM5gmwcW2PTHdYvPW4ll1UaZEkNKYlwJKfnx5LCyJxh1PblCyI+DIy5IyM9GAAAAAAAAAAAAAAAAMfsLaOOa6ahx5zE+2u7VSm6mhqWSkWVitJcq8prkiJCS4Nbq1IabIyNa0kZAOVsrA6zZmEWuF2rzkZM9tKo8xkvloMttZOR5TXPscZeQ26g/4yCHVaZzuxzrDuclYai5VQSnKPJYjXzWLJgk+Yaf8A03UKbfb+lp9s/eM6zj/iD2AXpWU5pC1pWuGZoqsZZZsbQkc9ifny21sJUZccoZjeqZq6Xl9lD5RPCvrlixnXcrJ9lSba0Ns509vYFzBdlG2npbNwocllCulPYuU8EXYuCAWIBJ1aPyGk+X17vXP6Z1PdMe3mov4jn7HEzkuSDL/2321f0h8WttZrrmU3X7+x2uhVjriWmc0ojcOm5Pgi9NadM3a0zUfBKWt5n2dT6TMkgK8A/KFocQlxtZKQoiUlST5IyP2GRj9AAiT7C/EPm9lVTeh7VeITVQZEclcoya6ZXw806X6UKKsuhSPY7IStKi6GeHNlvHMbXBNU5BkGPEg7xTLddTEsuUnZy3URYZKL3kch9rkveQ7rXuE1Ot8Ho8EozcVCo4LUNtx1RqceNKfWdcUfJqcWrqWpRmZmpSjMzMwHfoQhtCW20ElKSIkpIuCIi9xD9AAAAAAmm19dXNg81svV5RYWw6Ng0w1ur8qPcxiPqVWTTIj6mXO/QsyNTLhk4n9NK9PrvO6bZWHV2Z0aXW2JyVpdjPp6Xocltam5EZ1P6LrTqHGll7lIUNIJJiLRYN4gcsw1jlFXnNYjM4LXPqtzmFtw7LpL3JUS61wyLj5R11R91mZhWwAAAAAAAAAAAAGb2JndNrXDrHM7xLrjEFKEtRmE9T0yS4tLceM0n9J111bbSC96lpGY1Rrq5r3ndl7QKLN2HeMEmYtpfmx6aMZ9SayEZkXSy3261kRKecI3FfoJR1+XNFnPiBxPDX+V1eDVi8znNc+q5OfW5DreovelJIsnCI+flGmlF3QRlWwAAAAH5WhDiFNuIJSVEZKSZckZH7jH6ABEmGF+HjN62qhdDOq8vmpgx45q4RjN08vhlpov0YUpZ9CUexqQpCUl0PcN20Z7YWE1OyMHvMEvDcTCvILsNxxpRpcZNSfVdbUXBpcQrpWlRGRkpKTIyMh0ujsxtc71Tj+QZCSCvEsuV1ySC4SVnEdXFmEkvcRSGHeC9xAN2AAAAAAAAAAAAAAAAAAAAIhArWfEdli8lvCU9rXFLJyPS1iu7GQ2Udw0uWL5ex2My6k0MNnyla21vHyRMmWn8Qd/cUusJ1bjM1cS+yiVDxeqkN/PjSLCQiN6Qn9rCHVv/wBTJ9j9h7TF8apMMxqqxDGoDcGppITNfBjNlwllhpBIQgv6kpIgHaAAAAAACVbR19bV12W6tWxenM6qOSLCubV5bWUV6OTOC/7vOIuo47x923D6TPy1uJPd4Zl1Hn2KVWZ41JU/WXMVuXGUpJoWSVFz0LSfdC0nylSD7pUlRHwZGO6Ek1eyjB9tbC1cykmqyaqPnFO2RcJaKwW83OZQRe4pcdyQr+lO9p+wgrYAAAAAAAAAAAADJ7P2BE1piMjI3a1+0muPMwaurjqJL9lPfWTceM2Z9iNa1Fyo+yEktavVSZjqNVasdw5yfmWY2DV7n2RklV3bkk+hCCPlEKIlXdqI1yZIR2NR8uL5WtRjp5jKM88SUWHJSTtZq+jRZtoMuUqubM3mW19/02Ikd8i/o2B+33VsAAAAB8pUWLOjPQpsZqRHkNqaeZdQS0OIUXCkqSfYyMjMjI/aPqACK4rHX4fs0rNbm++7rzLJC2MWU8s1fF+wJCnDqzWr/wAM6lK1RuT+TUhbJdlMpK1DHbewJOzNc3eHNyCizpTBPVczguqDYsqJ6HKTyR8KakNtOF+1A+2qM3LZWtMXz1UQojt7VRpsiL740hbZG6yf7W3OtB9z7pPuYDKeIP5SvwOI5+QkbAx/zi93ycknW+f+q03/AG8DsduXNhTWev1QrN2G1JylDMskyFMtvM+gy1mh0y7GjqQlRkZGXKSPjsOL4k4E9/UNre1MVyTPxKVX5ZGZaLlx46yYzNW0kvebjbC2+Pf5nA2EuoxLYFfSXUllm1hMuN21Y8lxXlqNbKkpcLpMiWk23ldlckZK5454AeWrfxHbIyDENgRE3FdJNOGTMhpplXjN3jqIzjbyUJbasJ59Ni0pLqDKXHbbT6vV0cOJItfmHiC2Xrx2XhGRlSWGSvzatEGypMYsrFiJGmtSnOXayM85LlONehOp+ScQThLSsyaIlEVEheGbS0Kqm0nxUly4NhVP0b7Fhdz5qfg100GqGnz31+WwRoT0tI4QguokEklK57FGh9YooZ+PfA9ipmyltznpa7yeuwQ+32aW1ON45THllylsm3Uk2kzSgkkZkAwOqNlZxned4g9lcKyqXncfyBuXDcrJ9SxNcjz4bTU1MGaSXmSWhRrQlwlKQTik9a+6j5WT+JH4v7XPW/xi0W1xYRoXo9ptX0K9+V6PV+DPg9fyx9fqNed6/KPWT1dqNiep8Awdda7jNEqK9UxZUSO85MffdUiS6h6Qp1x1alPuOONpWp101OGojM1dz51wCU1Z5lE8R02Jc5pIn1E3FnJUGoaYJiJCJExCSV09SlOvGSjJTilccERJQgurqZ78l4gdUSGvyrkHI4q+Pb5CmYrii/q62Wf7SIUQ8cplZGnLTh/52RBVXJkeYvtHNwnDR089PzkkfPHPb28CcpX8a/E6S2PXia8xF2O+ovm+n20hlZI/522K4lGXuTLT/GIBWAAAAAAAAAAAAAEpwL5XxA7XkO/lW4OORUc+3yEsynEl/V1vPf2mYxmZ7TyvDchzuPVZRFjSHcmrq+vKfSWmQqYbOraecTEqq8yfkKM0rUpKFtpQSluqM+npVsVL+KnidNb/AKkTYeItR2FH830+pkPLNH/O4xYmoi96Yiv4pjQZNpbXWXSZE+3qZ7c2RYtWqptfczYEpEptj0dK23ozzbjZGz8mpKFElSTMlEfJgIbSba3Rn1nhtzittj1ZOmYdezLJNrXz24Dr0KwYZJaa5TzbzK3CJRfKuGuOSzJROnyR/pnxZ5tkeT1a8Pw6a/UEdImbWtYZc2jstM9hl511u1iJOHCKOiQlRofQo3CbVybRKSoVub4btNTYtfDPFZMVuqYlxIaoNxOiONRpT6X5DBLZeSo2XHEJNbRmaFJIkGXT6o7C30Xq28yCvySfjThSK30Y240exlR4Dxxv9WORCacTGkmzwXlm82s2+lPTx0lwHQYBmllVaYv8xtriuefqrTIXCk5HcqhQ222LGSltMiWpDpsMpQlKevoUSEpLhJkXAnr3iGtdlYXawajKtYeYq1o6t6XrvY/xhkxI82ehh5bplDjHFM2zUSHEmpXUZmnpNBGPRePY3S4pXqqqCF6LFXJkTFN+YtfLz7qnnVcrMz9Zxaz454LngiIuCDJcapMvo5eOZHATMr5qOh5o1qQfYyNKkrSZKQtKiJSVpMlJURGkyMiMBP8ARsywjys8wh+1sbKvxHJDrauRYzXpsooy4caR5Tkh5SnXjQt5ZEtxSl8dJGZ8D8eHz5OvzyI3+Qj7AyDyS93ykk3XOP8Aquuf28jY0OO4dqnE5LNaS6+pgpfsJsqbNelvLPg1uyJEl9a3nl8FypxxalGRF37EMr4bIE9jUNVe20VyNPy2VYZZJZdLhxk7OY9NQ0ovcbbb6G+Pd5fACngAAAAAAAAAAAAAAAAAAAlO6/lc10pDc/1eRsBzziP2fJ4/cPN8/wDVaa/t4Hx27mljhedUFkzcR4kKNjWQz32p0x1iC44wmKba3/LSs+E9SuDJClF1H0kZnwf18SRLqcErtiNoUr8H+QV2TyDIvmQGXfLnr/8ArBelq/8Arx255GyyrX2E5+hLuUUzVm2qvl1yep5wkKiyiQTyeEKIj6ibRwr2lxykyAea1702zk+O2NWi2hOXdVlOKNRJ/wAVLnEGnWZ0tKHGnodg4uQ42XQvlSVEh5KugiSZKMdxlPiQ2Xj75a7V8FuZfEtLGDMu6vB7i9grRGYivoUmrgPLlNm4U1pCjU+pDZoUfUvqSk6sjw5agKslVMnHZ89uc7AfkvWF7YTJTz0J03Yjq5Dz6nlONLPlKzX1ESUFzwhJF95egNXTMZiYquqt2Y8KQ7Lamxsiso9obzpcPOLsW30zHFOFwSzW8fWRESuSIiIM7pfO8nzrNrO0yKutKVUvEMesHKOYT7fwfJdcm+cRMvJQtCjNCSPqQlRklPUXbgcHGPEj8YNrlrf4xaLd5sJML0er2r6be/Jdfq/Bnwej5Yuj12vO9ThfrK6e9SxjXuGYW/5+K0DFYfwdEqSQwpRNpiRvM8htKOelJJ81zuREZ9Xcz4LjRAJTqs8yjbR2RVZdmki/NlVVIitkwUeLBQ607y1HZJSjQn1UmZqWtalcmauOlKV38j4p8MNn/wAXr/J/SOPf5NjSeVz/APve4/rMUSFjlNXXdnkUOH5dhcJYRNe8xZ+aTKVJb9Uz6U8EpXzSLnnvyJzjSyyvxH5fkDPrwcJoYWLNuF3L4QlLOdMb5/os/BZ/1rPnjguQrAAAAAAAAAAAAACU6i+V2jvGS7+WbzGviI59vkJxypcSX9XW89/aZiX5BuvPMOoJ8ehyqCmycyPJ3ElPxW6yuUcaLLUltpEOtUhbLBdSEnIccJtvhKCQrr5TTseWWKeJHLKKR6kfO6GDkVeZ9vNlQjOHOSX09LS6s/p9c/o79tcaD1XeyGZc+gmJdZkTZBqi3E2L5/pjhOSmnyZeST7DiyI1MO9TR8F6nYBGnt0bUTbZHsimlUTGPVWFY3lFlVWSpElTiH0ylvsROl1tEVZpSRk+pLpLNKUm2XHUXPrfEhnmSbJk0lXjU5zGl21lQLaawq6S5C9GQ6RWCroiOuWhTrRoNgiSpHmJ5dNSVIOnu+HXT70+FYrxZ8noMOBXElFtNQ1IiQuv0ViS2TxIlNNm4tRIeJaeo+oyNXBjnr0jrNzM1Z85QyV2qnHZBIVaSzgokuNm25JRC830VEhSDUk30tE6ZKUXV3PkMnT7Ln4Z4ZMU2Da3mLnMOiqlyJ2aZOqnguOOobJSn5ymXzStRqPjltRrUZFyXPIySNszd2V2KUScoxBFbe5Wuos5etdhO2qfIar35RMqsGGIj0V1bjaOUtcK6CL1+FmQ9B4/Q1OLUVfjVFF9FrauM3DiM+YpflstpJKE9SjNR8ERFyZmf0mODmeD43n9OVJk8SS6wh5Ehl2JOfhSozyeel1mTHWh5lZEZl1NrSrhSi54MyMMr4fb+5yDW7a7ywesJFbbWtQ3MfV1OyGIk55hpa1fpr8ttJKV7VGRmfczHB8MvqaukxkfkYuY5hEj/QTDWR2LbRF+zoQnj9g1s+ThumdbzrIo7dXjeJVb8xxCDM/LYZQpxZ8qMzUo+FGZqM1KUZmZmZmY6fQGMW+IabxSnyNg2LtyAVhbtH/u7CUtUmUnv7eH3nC59/ACgKSlRGlREZGXBkfvIRjU8o9Q5Svw8XSVtVJJem4DNc4JqRW8mtdWk/c9CI+lKfaqN5Si6jQ702gZ7O8DxzY2POY5k0Z1TPmokxpEd1TMmFJbPlqTHeTwpp5Cu6VpPku5dyMyMNCAjkXM9p6ibTV7So7LN8fYLpZy/H4BvTUN+74QrWSNzrIi7vRUuJX7TaZ9g2OIbl1PnvKMO2Njtq+g+l2KxYNnJZV/EcZMycbV9KVpIy+gBsgHWXeTY3jMRU/I8graqMgupT06W2w2RfSalmRCbv8AiJockUqu0hQzdl2BmaUyao/KpGT/AIz1qtPo/T7eSZN53t2bPkgGu2fsas1hij2RTYMmzmOuIhVNTD4OVaz3OSZiMEfY1rMvafqpSSlqMkpUZcDTeBWWC4vIeyeWzNyvJZ7t9kktkvUdnvEkjbbP2m0y0hmO3z38thvnvyOFgmrbiNkZ7K2pfsZHmJtrZhlHZNqtoo6+OtiC0ozURq4InJCzN1zjj1EcNppIAAAAAAAAAAAAAAwe5MCss6xeO9jEtmFleNT2r7G5bxeo1PZJRE24ftJp5pb0dzjv5b7nHfgc/WGxqzZ+KM5FCgyayY04uFbVMzgpVVPb4J6I+RdiWgz9peqpJpWkzSpJnrRNs71bcScjLZWq79jHMxJtDMwpDJu1t7HRz0MTmkmSjNPJk3IQZOt88eujltQUkBJGPETQ42pNdu+hm60sCMkqk2p+bSPH/GZtUJ9H6fZwTxsu9+7ZcGKRSZNjeTREz8cyCttYyy6kvQZbb7Zl9JKQZkA7MBjcv3LqfAuEZjsbHap9Z9LUV+wbKS8r+I2yRm44r+ihJmf0DHSsz2nt1tVXq2jssIx98ul7L8ggGzNW37/g+teInOsyPs9KS2lHtJp72APltiUe3spR4eKVK3ak0szc+mt8G1HreSWirUfvemkXSpPtTG81R9Jra6rOlKUkSUkRERcERe4hn8EwPHNc483jmMxnUs+auTJkSHVPSZslw+XZMh5XKnXlq7qWo+T7F2IiItCAAAAAAAAAAAAAAAAAAAAA+E6DDs4UitsYrUmLLaWw+y6klIcbURkpKiPsZGRmRl+0SbTVtI17Yfi75fOcVOpI63cSmyHDWq5oGzJLXyij5XJjEaWXiP1jImnfY72sAymxda41s2nYrL8pUeTAkpnVdnAe8idWS0kZIkRneD6FkRmRkZGlSVKQtKkqUkw1YCPM7I2Vq5RVm5MTmX9S0Rk1mWLVzkltTZcERz69vqkR3OOTUtlLrHY1GbJcILaYhtvVuwG/MwnYmOXhkZpU3BsmXXG1F2NK0ErqQoj7GlREZH2MgGtAdRkOX4niURU/KsoqKaMgupT1hNajNpL6TU4oiITh/wAQLOYcV+hMVmZ/Ke9VFwjqiY7G7dnHLFaTQ8ku3KYiX19y9UiPqINTtTZDGu6Jn0GEVrk1298HY3SoWSXLKwUkzQjn9BpJEbjrp9m20LWfs4P96k1+vW+FRqKdY/CdzKeftLyy4MvTrOS4bsl4iPulBuKMkI59RtLaC7JIdbgGqXaW+e2Pn10nJc7nRjiuWBMm1EroxqJRw69gzV5DHJJNRmpTjppJTi1cJSmiAAAAAAAAAAAAAACd7nwm/wAiqqrLcEQ0eZ4XO+F6RDrvlNTPUU3Iguq9hNyGFuN8n2Q55TnBm2Q0OAZ9jmy8XjZZjEh1UZ9S2XmH0eXIhyW1Gl6M+2fdt5tZKQtB9yMj9pcGNGJflmrL6qyaZs7TFjBqMosCR8L1lh1/BGQEhPSk5KWyNbMhKSJKJTZGokkSVodSSUpCoAJLE8R+IUziKzcVZYawtDX5RlkhJbrXl88F6PZoM4jpKPjpSbiXeFJ6m0GfApdTf0V9ETPo7qBYxlF1E9EkoebMvpJSTMgHPAYfLd36gwV1MXKdj0EKa4fSzAKah2a+r+K1GbNTzqv6KEGf7BlHch21uM1VuIUdrrnEnT6JWQXEcmLyW0ZFyUCEvk4pn3T50tKXEfosH2WQfPN5De7c9j6npnCkYrilhHsc3lIV8m9JaNL8OoSovnKU4Tb8hPsS0hDavy/azjosIwfF9dY1ExHD6tECshko0NkpS1uOLUanHXHFGa3HVrNSluLM1KUozMzMzMd6AAAAAzWWay1tnxEWda9xrIySXSXwtUx5nBfR8qhQAA6Cl8Ofh7xuUmdjuiNeVUlJ9SXoWLwWFkf0kpDRGKEhCG0JbbSSUpIiSki4Ii+ggAB+gAAAAAAAAAAAAAAAAAAAflaEOIU24klJURkpJlyRl9Bie3Xhz8PeSSlTsi0Rry1kqPqU9NxeC+sz+k1LaMwAB3+J6y1tgJGWC69xrHCUXSfwTUx4fJfR8khI0oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMrlup9WZ+55ud61xXJF8Enqt6aNMPgvYXLqFAADqaDw96DxSUmdi2j8AppKD6kvV+NQo60n9JKQ2RkKAAAAAAAAAAAAAAAAAAAAAAAPw600+0th9pDjbiTQtC0kaVJMuDIyP2kZCd2vhs8Ol7KOdd6C1xYSVH1G9LxWC8sz+nqU0ZgADT4pr3AcDaUxg+D4/jrSy6VIqaxmIlRfQZNJTyNAAAAAAD/9k=)\n","\n","Predicted values $ :  15 \\Rightarrow 1 , 16\\Rightarrow 1, 17\\Rightarrow 0, 18 \\Rightarrow 1, 19 \\Rightarrow 0 $ \n","\n","Lables $ :  15 \\Rightarrow 1 , 16\\Rightarrow 1, 17\\Rightarrow 0, 18 \\Rightarrow 0, 19 \\Rightarrow 1 $\n"," $$ Precision  \\Rightarrow T_p / (F_p + T_p)  = 2 / (1 + 2 ) = 66 \\% $$ \n"," $$ Recall \\Rightarrow T_p / (T_p + F_n)  = 2 / (2+ 1 ) = 66 \\% $$\n"," $$ Accuracy \\Rightarrow (T_p + T_n )/ (T_p + T_n + F_p + F_n) = 3 / 5 = 60 \\%$$\n","As we can see decision tree performs quite well considering that our train dataset is small. \n","It's confusion matrix is as below. \n","$$Confusion{Matrix} = \\begin {bmatrix} 2, 1 \\\\  1, 1\n","\\end {bmatrix}$$\n"]},{"cell_type":"markdown","metadata":{"id":"r4cVeh9_TPiL"},"source":["## Increasing Resistance \n","A decision tree continues to grow untill all the branches at last level are leaves, meaning the information gain of all the remaining features are zero. This can cause overfitting. Since overfitting happens when the model has lots of parameters, the more the decision tree grows the more it's possible to overfit. \n","\n","To avoid overfitting we can limit the tree's depth to some level, this means we stop the tree at some level.\n","\n","When overfitting happens, the model performs well on the training data but lacks general aspects, meanin low performance on the test data. Another way to avoid this, is to use forests. Forests use different features and data and decide for the lable from majoritie's rule. This can also help with the generalization of our model. "]},{"cell_type":"markdown","metadata":{"id":"xH4K8_mYKIpp"},"source":["# Second Problem : \n","\n"]},{"cell_type":"markdown","metadata":{"id":"6LyY-SJ96RVp"},"source":["## Decision Tree with ID3:"]},{"cell_type":"code","metadata":{"id":"getfcUXvcChq","executionInfo":{"status":"ok","timestamp":1652198410017,"user_tz":-270,"elapsed":15,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["from sklearn import preprocessing\n","#  We can use the encoder from this library to encode the lables into numbers \n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ljIQLygLJt2","executionInfo":{"status":"error","timestamp":1652198410018,"user_tz":-270,"elapsed":15,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}},"colab":{"base_uri":"https://localhost:8080/","height":375},"outputId":"f8e5267c-3b48-4e18-80f5-30848a07e00d"},"source":["data = pd.read_csv(\"prison_dataset.csv\")\n","headers = data.columns\n","data = data.to_numpy()\n","temp = data.T\n","lables = temp[len(temp)-1]\n","input = temp[0:len(temp)-1].T\n","lables, input = shuffle(lables, input)\n","#  How to transform lables to numerical values\n","encoder = preprocessing.LabelEncoder()\n","encoder.fit(input[:, 0])\n","X_temp = encoder.transform(input[:, 0])\n","encoder.fit(input[:, 1])\n","X = encoder.transform(input[:, 1])\n","values = [X_temp, X]\n","for i in range(2, len(input[0,  :])) :\n","  encoder.fit(input[:, i])\n","  X = encoder.transform(input[:, i])\n","  values = np.concatenate((values, [X]), axis= 0)\n","#  Split data into train and test subsets \n","test_lables  = lables[0:int(len(lables)* 0.2)]\n","test_data = values[:, 0:int(len(lables)* 0.2)]\n","train_lables = lables[int(len(lables)* 0.2): len(lables)]\n","train_data = values[ :, int(len(lables)* 0.2): len(lables)]\n","headers = headers.to_numpy()\n","headers = headers[0: len(headers)-1]\n","#  Removing target header from the headers "],"execution_count":9,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-8cc7fd7f32d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prison_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prison_dataset.csv'"]}]},{"cell_type":"code","metadata":{"id":"8CCKVZO3bzGg","executionInfo":{"status":"aborted","timestamp":1652198410017,"user_tz":-270,"elapsed":11,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["decision_tree = build_tree(train_lables, train_data, -1 , 0, headers, {})\n","decision_tree[\"name\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ig_kLI90Yoz2","executionInfo":{"status":"aborted","timestamp":1652198410018,"user_tz":-270,"elapsed":12,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["def predict (headers, test_data, decision_tree ) :\n","  predict_test = dict(zip(headers, test_data))\n","  tree = decision_tree \n","  predicted_lables = []\n","  for j in range(len(test_data[0])):\n","    #  Navigating the tree for each data \n","    tree = decision_tree\n","    depth = 0 \n","    while not (\"lable\" in tree.keys()) :\n","      #  We must follow the tree until the subset we are at has a lable attribute \n","      tree = tree[\"children\"][predict_test[tree[\"name\"]][j]]\n","      #  Following of the treee happens by changing from parent to child \n","      #   using the value of the test_data's attribute \n","      #  Each test data has the same attributes as our tree \n","      #   so at each level of the tree the next child will be the \n","      #  branch value of the tree that is the same as the test_data's value for that attribute\n","      depth += 1\n","    predicted_lables.append(tree[\"lable\"])\n","  predicted_lables = np.asarray(predicted_lables)\n","  return predicted_lables\n","def evaluate (predicted_lables, test_lables) :\n","  tp = np.dot(predicted_lables, test_lables)\n","  tn = np.dot(np.subtract(predicted_lables, 1), np.subtract(test_lables, 1)) \n","  fn = np.dot(np.subtract(predicted_lables, 1), test_lables) * -1 \n","  fp = len(predicted_lables) - tp - tn - fn \n","  conf_mat = [[tp, fp], [fn, tn]]\n","  print (\"confusion matrix is :\", conf_mat)\n","  print (\"Accuracy is\", (tp + tn) /(len(predicted_lables)))\n","  print (\"Precision is \", tp/ (tp + fp))\n","  print (\"Recall is \", tp/ (tp + fn))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBG9mJUCxkCf"},"source":["## Decision tree with different maximum depths \n","Predictions are made with different maximum depths for each tree. \n","The results are as followed. \n","maximum depth = 3 \n","$$ ConfusionMatrix = \\begin {bmatrix} tp = 1106, tn = 1082 \\\\ fn = 637, fp = 259 \\end {bmatrix}$$\n","$$ Accuracy \\Rightarrow 0.709468 $$ \n","$$ Precision \\Rightarrow  0.810256 $$\n","$$ Recall \\Rightarrow  0.634538152 $$\n","maximum depth = 4 \n","$$ ConfusionMatrix = \\begin {bmatrix} tp = 1108, tn = 1101 \\\\ fn = 640, fp = 235 \\end {bmatrix}$$\n","$$ Accuracy \\Rightarrow 0.7162 $$\n","$$ Precision \\Rightarrow  0.82501 $$ \n","$$ Recall \\Rightarrow 0.633867$$\n","maximum depth = 5 \n","$$ ConfusionMatrix = \\begin {bmatrix} tp = 13523, tn = 879 \\\\ fn = 399, fp = 453 \\end {bmatrix}$$\n","$$ Accuracy \\Rightarrow  0.7237354$$ \n","$$ Precision \\Rightarrow  0.7491694$$ \n","$$ Recall \\Rightarrow  0.7722$$\n","maximum depth = 6 \n","$$ ConfusionMatrix = \\begin {bmatrix} tp = 1259, tn = 977 \\\\ fn = 460, fp = 388 \\end {bmatrix}$$\n","$$ Accuracy \\Rightarrow 0.7250324 $$\n","$$ Precision \\Rightarrow  0.76442 $$\n","$$ Recall \\Rightarrow  0.73240255 $$\n","Generally when we have a larger maximum depth the accuracy increases, since more training is done on the data and more parameters are avaialble. As it's clear from the results precision and recall don't necessarily increase. "]},{"cell_type":"code","metadata":{"id":"BlCL-44xsYwL","executionInfo":{"status":"aborted","timestamp":1652198411247,"user_tz":-270,"elapsed":1240,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["# Bootstraping data set : selecting random rows of data from the dataset but with replacement \n","# meaning rows can be duplicates \n","# k is the number of trees \n","import random \n","k = 4 \n","headers = np.asarray(headers)\n","predicted = []\n","for i in range(k):\n","  column = np.random.randint(len(train_data[0]), size=(len(train_data[0])))\n","  row = np.asarray(random.sample(range(10),  4))\n","  #  For each tree we only use 4 rows of the attributes of the whole dataset\n","  boot_data = []\n","  for i in range(4):\n","    boot_data.append(train_data[row[i]][column])\n","  decision_tree = build_tree(train_lables[column], np.asarray(boot_data), -1 , 0, headers[row], {})\n","  predicted.append(predict(headers, test_data, decision_tree))\n","#  Reshaping the predicted values to an array with k rows each for one of our trees\n","predicted = np.asarray(predicted).reshape(k, len(test_lables))\n","count_0 = np.count_nonzero(predicted == 0, axis=0)\n","count_1 = np.count_nonzero(predicted == 1, axis=0)\n","counts = np.asarray([count_0, count_1]).T\n","final_predicted = np.argmax(counts, axis = 1)\n","#  Each final lable is assigned by the majority rule \n","evaluate(final_predicted, test_lables)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LaXTHI3Vkzuf"},"source":["## Random Forests \n","In comparison to the decision tree with the same maximum depth as our random forest, the random forest has a better performance, this we knew already but here the results are different. The reason why random forests generally perform better is that decision trees are highly sensitive to the data they are trained with, even if we have the same data but different sort of the data we may probably end up with different decision trees. This sensitivity reduces generalization of our model. When training a random forest we constantly change the structure of our training data, this can cause more generalization and reduce the sensitivity of this algorithm. The more our model is generalized the better it performs on the test data, but here the results are fairly the same. This is because of the data.The prison dataset's fiscal year release attribute has a high correlation with the traget arrtibute. Other attributes on the other hand have lower correlation with the target, in this case other attributes not really help on the decision we make and if both a tree and forest use this attribute in their decision making procedure, both will have a fairly correct lable for the test data. This is the main reason that both algorithms have approximately same results."]},{"cell_type":"markdown","metadata":{"id":"yvMd-GIOf-ra"},"source":["## Built-in Library Results \n","  In comparison to the results from our own implementation, both are in the same range and may differ only in 0-5 percentage. \n","  As stated above, they both have approximately same results. "]},{"cell_type":"code","metadata":{"id":"y1XkoEFNWkqO","executionInfo":{"status":"aborted","timestamp":1652198411248,"user_tz":-270,"elapsed":1241,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","classifier = RandomForestClassifier(max_depth=3, random_state=0)\n","classifier.fit(train_data.T, train_lables.astype(int))\n","predicted_lables = classifier.predict(test_data.T)\n","\n","print(confusion_matrix(test_lables.astype(int), predicted_lables))\n","print(classification_report(test_lables.astype(int),predicted_lables))\n","print(accuracy_score(test_lables.astype(int), predicted_lables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"In2c9sYD3PtH"},"source":["# Third Problem"]},{"cell_type":"markdown","metadata":{"id":"6KtcOfRo6Jh5"},"source":["## KNN "]},{"cell_type":"code","metadata":{"id":"cA5XDd3n3ciU","executionInfo":{"status":"aborted","timestamp":1652198411248,"user_tz":-270,"elapsed":1240,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["csvreader = pd.read_csv(\"wine.csv\")\n","headers = csvreader.columns\n","lables = csvreader[headers[0]]\n","data = np.asarray(csvreader[headers[1:]])\n","lables, data = shuffle(lables, data)\n","test_lables  = lables[0:int(len(lables)* 0.2)]\n","test_data = data[0:int(len(lables)* 0.2), :]\n","train_lables = lables[int(len(lables)* 0.2): len(lables)]\n","train_data = data[int(len(lables)* 0.2): len(lables), :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66p0-YF8DQG2","executionInfo":{"status":"aborted","timestamp":1652198411249,"user_tz":-270,"elapsed":1241,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["def distance (data_1, data_2):\n","  x_1 = np.asarray(data_1)\n","  x_2 = np.asarray(data_2)\n","  dist = np.sqrt(np.dot(np.subtract(x_1, x_2), np.subtract(x_1, x_2)))\n","  #  Computing the euclidean distance of two data \n","  return dist\n","\n","def KNN (old_data, new_data, k, lables) :\n","  dist = []\n","  for i in range(len(old_data[:,0])):\n","    dist.append(distance(old_data[i,:], new_data))\n","  indx = np.argsort(dist)\n","  # Sorting the distances in descending order\n","  lables_tmp = lables[indx[0:k]]\n","  one_ = len(np.where(lables_tmp == 1)[0])\n","  two_ = len(np.where(lables_tmp == 2)[0])\n","  three_ = len(np.where(lables_tmp == 3)[0])\n","  final_lable = np.argmax([one_, two_, three_])+ 1\n","  #  Final lable will be the lable most common in the k \n","  #   lables nearest to the test data\n","  return final_lable\n","\n","def predict (train_data, test_data, train_lables, k) :\n","  y = []\n","  for i in range(len(test_data)) :\n","    y.append(KNN(train_data, test_data[i, :], k, train_lables))\n","  return y\n","\n","\n","def confusion_matrix (y_pred, y_test, lable, if_print) : \n","  y_t = np.zeros(len(y_test), dtype = np.int8)\n","  y_t[np.where(y_test == lable)[0]] = 1\n","  y_p = np.zeros(len(y_test), dtype = np.int8)\n","  y_p[np.where(y_pred == lable)[0]] = 1\n","  precision = float('inf')\n","  recall = float('inf')\n","  tp = len(np.where(np.multiply(y_t, y_p) != 0 )[0])\n","  tn = len(np.where(np.multiply(np.subtract(y_t, 1), np.subtract(y_p, 1)) != 0 )[0])\n","  fn = len(np.where(np.multiply(np.subtract(y_p, 1), y_t) != 0)[0])\n","  fp = len(y_test) - tp - tn - fn \n","  if ((tp + fp) != 0) :\n","    precision = tp / (tp+fp)\n","  if ((tp + fn) != 0) :\n","    recall = tp / (tp + fn)\n","  f1 = 2 * precision * recall / (precision + recall)\n","  accuracy = (tp + tn)/ len(y_test)\n","  if if_print   :\n","    print(\"fp\", fp, \"fn\", fn, \"tp\", tp, \"tn\", tn)\n","  return precision, recall, accuracy, f1\n","y_pred = np.asarray(predict(train_data, test_data, train_lables.iloc[:].values, 5))\n","test_lable = np.asarray(test_lables.iloc[:].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaER-8y0RKdn","executionInfo":{"status":"aborted","timestamp":1652198411250,"user_tz":-270,"elapsed":1242,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["results = []\n","results.append(confusion_matrix(y_pred, test_lable, 1, True))\n","results.append(confusion_matrix(y_pred, test_lable, 2, True))\n","results.append(confusion_matrix(y_pred, test_lable, 3, True))\n","\n","print(\"result for class 1 :\", \"precision:\", results[0][0], \"recall:\", results[0][1], \"accuracy:\", results[0][2], \"f1:\", results[0][3])\n","print(\"result for class 2 :\", \"precision:\", results[1][0], \"recall:\", results[1][1], \"accuracy:\", results[1][2], \"f1:\", results[1][3])\n","print(\"result for class 3 :\", \"precision:\", results[2][0], \"recall:\", results[2][1], \"accuracy:\", results[2][2], \"f1:\", results[2][3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mQHOtn1Uk5-","executionInfo":{"status":"aborted","timestamp":1652198411251,"user_tz":-270,"elapsed":1243,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["def find_opt_k () :\n","  #  computing accuracy for a range of neigbors to find the best value \n","  test_lable = np.asarray(test_lables.iloc[:].values)\n","  accuracy_1 = []\n","  accuracy_2 = []\n","  accuracy_3 = []\n","  for i in range(0, 20, 2) : \n","    y_pred = (np.asarray(predict(train_data, test_data, train_lables.iloc[:].values, i+1)))\n","    accuracy_1.append(confusion_matrix(y_pred, test_lable, 1, False)[2] *100)\n","    accuracy_2.append(confusion_matrix(y_pred, test_lable, 2, False)[2] *100)\n","    accuracy_3.append(confusion_matrix(y_pred, test_lable, 3, False)[2] *100)\n","  plt.pyplot.plot(range(0, 20,2), accuracy_1)\n","  plt.pyplot.plot(range(0, 20,2), accuracy_2)\n","  plt.pyplot.plot(range(0, 20,2), accuracy_3)\n","find_opt_k()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eK5i4qsmvcAb"},"source":["As we can see in the plot above, 9 can be a good value for the number of neighbors."]},{"cell_type":"code","metadata":{"id":"_bbX7UCVbpbC","executionInfo":{"status":"aborted","timestamp":1652198411251,"user_tz":-270,"elapsed":1242,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["!pip install metric_learn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USFNOkAB4KWf"},"source":["## LMNN "]},{"cell_type":"markdown","metadata":{"id":"0cSk7dGtFtIF"},"source":["### Description \n","Large margin nearest neighberhood, is another metric learning algorithm used for KNN classification. It's main difference is how it measures distance. This algorithm uses Mahalanobis method for computing distance. Mahalabonis is a way of measuring what distance point P has from a distribution. In another way how many standard deviations is P away from the mean of D. In LMNN, mahalabonis is used to save K nearest neighbors form test lable x which have the same lable as x and save other neighbors with different lables by a margin. In this algorithm, the following equation is optimized to find the best condition ( where k nearest lables with same lable as x and others with a margin are saved ). \n","$$ min_L∑_{i, j} η_{i,j}||L(x_i−x_j)||^2+c∑_{i,j,l} η_{ij}(1−y_{ij})[1+||L(x_i−x_j)||^2−||L(x_i−x_l)||^2])$$\n","$$ x_i \\Rightarrow DataPoint $$\n","$$ x_j \\Rightarrow KNearestNeighbor $$\n","$$ x_l \\Rightarrow LargeMarginNeighbors $$\n","$$ η_{i,j} , y_{i,j} 𝜖 \\{0,1\\} $$\n","If $y_{i,j} = 0$ then $x_i, x_j$ belong to different classes. "]},{"cell_type":"code","metadata":{"id":"qZ7s5XvvavK0","executionInfo":{"status":"aborted","timestamp":1652198411251,"user_tz":-270,"elapsed":1242,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["from metric_learn import LMNN\n","from sklearn.neighbors import KNeighborsClassifier\n","k_test = 5 \n","lmnn = LMNN(k=5, learn_rate=1e-6,  random_state= 0)\n","lmnn.fit(train_data, train_lables)\n","knn = KNeighborsClassifier(metric = lmnn.get_metric())\n","knn.fit(train_data, train_lables)\n","predicted_lib = knn.predict(test_data)\n","results = []\n","results.append(confusion_matrix(predicted_lib, test_lable, 1, True))\n","results.append(confusion_matrix(predicted_lib, test_lable, 2, True))\n","results.append(confusion_matrix(predicted_lib, test_lable, 3, True))\n","\n","print(\"result for class 1 :\", \"precision:\", results[0][0], \"recall:\", results[0][1], \"accuracy:\", results[0][2], \"f1:\", results[0][3])\n","print(\"result for class 2 :\", \"precision:\", results[1][0], \"recall:\", results[1][1], \"accuracy:\", results[1][2], \"f1:\", results[1][3])\n","print(\"result for class 3 :\", \"precision:\", results[2][0], \"recall:\", results[2][1], \"accuracy:\", results[2][2], \"f1:\", results[2][3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXeLxMBtzEuH","executionInfo":{"status":"aborted","timestamp":1652198411251,"user_tz":-270,"elapsed":1242,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["test_lable = np.asarray(test_lables.iloc[:].values)\n","accuracy_1 = []\n","accuracy_2 = []\n","accuracy_3 = []\n","for k in range(1, 20, 2) : \n","  knn = KNeighborsClassifier(n_neighbors = k )\n","  knn.fit(lmnn.transform(train_data), train_lables)\n","  predicted_lib = knn.predict(test_data)\n","  accuracy_1.append(confusion_matrix(predicted_lib, test_lable, 1, False)[2] *100)\n","  accuracy_2.append(confusion_matrix(predicted_lib, test_lable, 2, False)[2] *100)\n","  accuracy_3.append(confusion_matrix(predicted_lib, test_lable, 3, False)[2] *100)\n","plt.pyplot.plot(range(1, 20,2), accuracy_1)\n","plt.pyplot.plot(range(1, 20,2), accuracy_2)\n","plt.pyplot.plot(range(1, 20,2), accuracy_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DD9GY3VeCurt"},"source":["## Finding Best Number Of Neighbors\n","As it can be deduced from the plot, eventually all lables converge to a maximum value after some number of k. We can check this number for each lable and when the last lable converges the maximum k can be chosen as the best number of neighbors to use. "]},{"cell_type":"markdown","metadata":{"id":"rm3-baRy4N5U"},"source":["## MLKR"]},{"cell_type":"code","metadata":{"id":"v5khJpJR4Osa","executionInfo":{"status":"aborted","timestamp":1652198411251,"user_tz":-270,"elapsed":1242,"user":{"displayName":"qazale keshavarz kalhori","userId":"17970473607893924114"}}},"source":["from metric_learn import MLKR\n","from sklearn.neighbors import KNeighborsClassifier\n","k_test = 5 \n","mlkr = MLKR()\n","mlkr.fit(train_data, train_lables)\n","knn = KNeighborsClassifier(metric = mlkr.get_metric(), n_neighbors= k_test)\n","knn.fit(train_data, train_lables)\n","predicted_lib = knn.predict(test_data)\n","results = []\n","results.append(confusion_matrix(predicted_lib, test_lable, 1, True))\n","results.append(confusion_matrix(predicted_lib, test_lable, 2, True))\n","results.append(confusion_matrix(predicted_lib, test_lable, 3, True))\n","\n","print(\"result for class 1 :\", \"precision:\", results[0][0], \"recall:\", results[0][1], \"accuracy:\", results[0][2], \"f1:\", results[0][3])\n","print(\"result for class 2 :\", \"precision:\", results[1][0], \"recall:\", results[1][1], \"accuracy:\", results[1][2], \"f1:\", results[1][3])\n","print(\"result for class 3 :\", \"precision:\", results[2][0], \"recall:\", results[2][1], \"accuracy:\", results[2][2], \"f1:\", results[2][3])"],"execution_count":null,"outputs":[]}]}