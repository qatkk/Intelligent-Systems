# Reinforcement Learning
In most problems, the complete model data is not known to the agent and the agent can only form the MDP model needed to solve the problem through sampling and input data.
In this type of problem, control and non-model methods are used to determine the appropriate policy for the agent.
One of these methods was Q-learning. The q function is a probabilistic mapping of the value of a state and action to the value of the value function. In this way, we calculate the mathematical expectation of possible rewards, provided they are in state S and move using move a. (Note that v only depends on the current state).
In this way, in this method of learning, we can define and optimize the probabilities and how the agent works independently of the existing policy.
These calculations and updates related to q values are obtained using the following equation.

<img width="554" alt="image" src="https://user-images.githubusercontent.com/43328710/175523052-e7732063-0067-4bad-9147-55220e9b537f.png">

In this way, we quantify the matrix related to the possible states and operations of each construct, and improve it each time the algorithm is repeated according to the above equation.
In this way, we implement the following two modes and bring the reports related to the code of each in the following sections.

## Without Reward 
In each iteration of the algorithm, we select the corresponding action according to the epsilon-greedy algorithm. The corresponding code is shown in figure below.

<img width="469" alt="image" src="https://user-images.githubusercontent.com/43328710/175523311-942080a6-87cc-4946-88a9-547c2160faa7.png">

We then set the value of the action and the corresponding state according to Equation 6-6, and we do this until our final reward value in each episode of the repetition algorithm converges to a fixed and minimum value.
In this way, the following diagram is obtained related to the final reward of each algorithm repetition episode.

- Plot of final reward in each iteration 

<img width="433" alt="image" src="https://user-images.githubusercontent.com/43328710/175523529-068bc249-b73d-46f4-ae55-e2a1b5c755b5.png">

<img width="554" alt="image" src="https://user-images.githubusercontent.com/43328710/175523052-e7732063-0067-4bad-9147-55220e9b537f.png">
After obtaining the optimal values of the matrix q, we can move the factor with a greedy algorithm according to the resulting matrix in the world and reach the final point.
   The above description can be implemented in the attached code according to the following function.
   
  <img width="451" alt="image" src="https://user-images.githubusercontent.com/43328710/175523647-c62193e2-b413-4fb9-9cd2-a4f435e8dca8.png">

## With Reward 
In the previous section, the reward of moving in different directions is considered for factor zero. In this way, due to the lack of punishment for moving without a cause in the map, in many cases, at the beginning of the training, the agent even moves away from the end point. For this reason, considering a negative reward for movement causes the agent to reach the end point faster and with fewer transitions. Similarly, in this part, by changing the part specified in the given code above, the reward is 1- for moving, which we see at the end, it makes a much faster convergence according to the graph for us.

<img width="420" alt="image" src="https://user-images.githubusercontent.com/43328710/175523884-dd4cff57-9c00-4369-bc9c-71b5cbe72920.png">
