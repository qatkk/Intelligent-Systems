# Theoretic Solution 

The policy iteration algorithm is implemented and repeated in two stages. We first evaluate the current policy and then greedily improve it.

<img width="571" alt="image" src="https://user-images.githubusercontent.com/43328710/175524135-cd5300a2-a7e5-4aab-a9dc-7f2219b1cf07.png">

In this way, we first set the policy values and the policy value function to zero and continue the calculations.

<img width="301" alt="image" src="https://user-images.githubusercontent.com/43328710/175524213-e4949148-d3ca-471e-8846-afaae86b9e9a.png">
$$V(1,3) = 0.2(0.6 \times 3 + 0.2 \times 3) \times \frac{1}{3} = 0.16$$
$$V(2,3) = 0.2(0.6 \times -2 + 0.2 \times -2 + 0.2 \times -2 + 1 \times 0 ) \times \frac{1}{4} = -0.1$$
$$V(3,4) = 0.2(0.6 \times -2 + 0.2 \times -2 ) \times \frac{1}{4} = -0.09$$

<img width="292" alt="image" src="https://user-images.githubusercontent.com/43328710/175524290-21411a22-984b-4af1-857a-3f592851389e.png">

Probability Â¼ is considered because the policy is initially considered uniform.
Again, we calculate values and policies with respect to high values.

$$V(1,3) = 0.16 + 0.2(0.6 \times 3+0.2 \times -0.1 )=0.516$$
$$V(2,3) = -0.1 +  0.2(0.6 \times 0.16 + 0.2 \times -2) = -0.404$$
$$V(3,4) = -0.09 + 0.2(0.2 \times -2) = -0.17$$
$$V(1,2) = 0.2(0.6 \times 0.16 ) = 0.0192$$
$$V(2,2) = 0.2(0.2 \times -0.1) = -0.004$$
$$V(3,3) = 0.2(0.6 \times -0.09 + 0.2 \times -0.1 )= -0.0148$$

<img width="292" alt="image" src="https://user-images.githubusercontent.com/43328710/175524450-447b96dd-0a97-44f7-8d8f-3b94f38d0b75.png">

These two steps are repeated until the mathematical expectation of the reward values for all modes does not change in two consecutive stages, or in other words, these values become stable. In this way, this method achieves an optimal policy.
Model-based methods, one of which is the policy repetition method, have theoretically proved convergence, which is mentioned in Sutton-Barto[1].

## Reference 
<a id="1">[1]</a> 
Reinforcement Learning: An Introduction
R. Sutton, and A. Barto. 
The MIT Press, Second edition, (2018)
