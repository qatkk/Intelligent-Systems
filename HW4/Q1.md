# Theoretic Analysis of Single-link and K-nearest Neighbor Methods
In this question, we intend to manually implement the ka-mean clustering method and the single-link hierarchical clustering method in one step. The problem data are shown in Tables 1-1 and 2-1, respectively.
<img width="611" alt="image" src="https://user-images.githubusercontent.com/43328710/175349841-b80d78b8-3316-4c80-bcba-171e0389961d.png">

For better intuition, the points on the coordinate page are drawn in Figure below..
<img width="484" alt="image" src="https://user-images.githubusercontent.com/43328710/175350172-46e7c582-167e-4081-b7d7-0410298ffa20.png">
## K-means Algorithm Description 
Ka-mean algorithm is an unsupervised clustering algorithm. In this algorithm, we cluster different data based on similarity metrics and consider a center or representative for each cluster. One of the challenges of this algorithm is how to determine the number of clusters, which we will discuss in the following sections.
This algorithm has two parts. First, we randomly select the data point and consider them as the representatives of each cluster. Then we move on to the rest of the data and calculate the similarity metric (in Ka-mean, the similarity metric is the second power of the Euclidean distance) for each point up to the centers of the clusters. At this point, each point is assigned to the cluster with the most similarity / least distance.
After performing the previous steps and assigning the data to the clusters, we calculate the center of the clusters again and repeat the previous steps. We do this as long as the centers of the clusters do not change, so that we have reached a local minimum or a general minimum in the cost function.
Since this algorithm is randomly set, repeating the algorithm does not necessarily give us better behavior.
## Calculations 
The primary random centers are selected A and D, respectively.
<img width="605" alt="image" src="https://user-images.githubusercontent.com/43328710/175353740-8276ceb1-cb3f-4658-a9c4-3ec32d206243.png">
In this way, clustering after the first step and the centers of the clusters are calculated as follows.
$$Cluster 1 \Rightarrow A, B, C$$
$$Cluster 2 \Rightarrow E, D$$
New Centroids : $$Second \Rightarrow  (\frac{3+2}{2}, \frac {5+4}{2} = (2.5, 4,5)$$
 $$First \Rightarrow  (\frac{1+1 + 0}{3}, \frac {1+ 0+ 2}{3}) = (\frac{2}{3}, 1)$$
In the next step, we calculate the distance between the points and the new centers, and again we assign each point to a cluster.

<img width="651" alt="image" src="https://user-images.githubusercontent.com/43328710/175354869-da0973c1-3669-4ff8-bed6-bc9327f88f22.png">

$$Cluster 1 \Rightarrow A, B, C$$
$$Cluster 2 \Rightarrow E, D$$
New Centroids : $$Second \Rightarrow  (\frac{3+2}{2}, \frac {5+4}{2} = (2.5, 4,5)$$
 $$First \Rightarrow  (\frac{1+1 + 0}{3}, \frac {1+ 0+ 2}{3}) = (\frac{2}{3}, 1)$$
Since the coordinates of the centers of the clusters did not change, the algorithm is stable and the clusters are specified according to the above values. 
 ## Hierarchical clustering (single-link) 
 
Clustering is a hierarchy of unsupervised clustering algorithms. Algorithms of this category hierarchically cluster the data in two ways. One method is that initially, each data is considered a cluster, and with the development of the algorithm based on the similarity criteria of different data are placed in related clusters. Another method is that, at first, all the data is considered a cluster, and the dendrogram of the algorithm is formed by separating the same data so that, in the end, the clusters include each of the data. In this part, we implement the single-link hierarchical algorithm manually and use it to cluster the data so that all the data are in a cluster at the end (the dendrogram of this algorithm starts from each data and ends with all data being in the same cluster).
<img width="603" alt="image" src="https://user-images.githubusercontent.com/43328710/175357782-f6e0b155-cb9f-4171-8780-4374b4892bc6.png">
First, we select the points that are less distant from each other and place them in a cluster; then, we re-create the table above for the new clusters. The new distances to the cluster are formed in such a way that we consider the minimum distance of a point to the points inside the cluster as the distance of the point to that cluster.
- Round two of the calculations 
 <img width="603" alt="image" src="https://user-images.githubusercontent.com/43328710/175357988-0b3327c0-248e-4b34-a9c0-7980c8e5842c.png">
 At this stage, the distances of points 1 and 4 to the formed cluster are the same, and thus all are located in one cluster, and at the end, the algorithm ends by joining point 3 to the clusters. Figure 2-1 shows the dendrogram associated with this clustering method and the calculations performed.

- Final dendogram
<img width="613" alt="image" src="https://user-images.githubusercontent.com/43328710/175358113-0e6a6c7e-2807-48a6-9296-679aabbb4922.png">

